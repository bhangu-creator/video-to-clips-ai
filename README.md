# Video to Clips AI - Automated Video Clipping Pipeline

A Next.js application that transforms long-form videos into shareable short clips using AI-powered moment detection.

## ğŸ¯ Overview

This application provides an end-to-end pipeline for:
- Uploading long-form videos (10-30 minutes)
- Generating timestamped transcripts using Whisper
- AI-powered highlight detection from transcripts
- Automated clip generation in both horizontal (16:9) and vertical (9:16) formats
- Metadata storage in PostgreSQL

---

## ğŸš€ Setup Instructions

### Prerequisites
- Node.js 18+ 
- PostgreSQL database (Neon recommended)
- Redis instance (Upstash or local)
- FFmpeg installed on your system
- Groq API key (free tier available)

### Installation

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd video-to-clips-ai
```

2. **Install dependencies**
```bash
npm install
```

3. **Set up environment variables**

Create a `.env` file in the root directory:

```env
# Database
DATABASE_URL="postgresql://user:password@host:5432/dbname"

# Redis (for background jobs)
REDIS_URL="redis://localhost:6379"

# Groq API (for transcription and AI analysis)
GROQ_API_KEY="your_groq_api_key_here"
```

4. **Run database migrations**
```bash
npx prisma generate
npx prisma db push
```

5. **Start the development server**
```bash
npm run dev
```

6. **Start background workers** (in separate terminals)
```bash
# Terminal 2: Transcript worker
npm run worker:transcript

# Terminal 3: Clips worker  
npm run worker:clips
```

Add these scripts to your `package.json`:
```json
{
  "scripts": {
    "dev": "next dev",
    "worker:transcript": "tsx lib/worker/transcript.worker.ts",
    "worker:clips": "tsx lib/worker/clips.worker.ts"
  }
}
```

---

## ğŸ—ï¸ System Architecture

### High-Level Pipeline

```
Video Upload â†’ Transcription â†’ Highlight Detection â†’ Clip Generation
     â†“              â†“                  â†“                    â†“
  Storage      AI Whisper         AI Analysis           FFmpeg
     â†“              â†“                  â†“                    â†“
  Postgres    Chunked Queue      Store Metadata      Multi-format
```

### Architecture Components

#### 1. **Video Upload & Storage**
- Videos are uploaded via Next.js API routes
- Files stored locally in `uploads/original/` directory
- Metadata (filename, filepath, duration) saved to PostgreSQL

#### 2. **Transcription Pipeline (BullMQ Worker)**
- **Chunking Strategy**: Large videos split into 120-second chunks to handle Groq's API limits
- **Worker Process**: `transcript.worker.ts` processes each video asynchronously
- **Audio Extraction**: FFmpeg extracts audio from video â†’ MP3 format
- **Audio Splitting**: Split into 2-minute chunks for optimal API performance
- **Whisper Transcription**: Each chunk sent to Groq's Whisper API
- **Database Storage**: Transcript chunks stored with timestamps in PostgreSQL

**Why 120-second chunks?**
- Stays well within Groq's free tier limits (30 requests/min)
- Optimal balance between API calls and context length
- 2.5 second delay between chunks ensures <24 requests/min

#### 3. **AI Highlight Detection**
Multi-phase AI pipeline using Groq's Llama 3.1:

**Phase 1: Candidate Extraction**
- Transcript chunked into 4-segment pieces (8 minutes of content each)
- Each chunk analyzed for potential highlights
- Returns candidates with strength scores (0.0-1.0)

**Phase 2: Ranking & Deduplication**
- Filters clips shorter than 15 seconds
- Removes overlapping highlights (>80% overlap)
- Sorts by AI-assigned strength scores

**Phase 3: Final Selection**
- Top 10 candidates sent for final AI evaluation
- AI selects 3-5 best highlights
- Optimizes titles for clarity and engagement

#### 4. **Clip Generation (BullMQ Worker)**
- **Worker Process**: `clips.worker.ts` processes clip jobs
- **FFmpeg Processing**: Generates two versions per highlight:
  - **Horizontal (16:9)**: 1920x1080, maintains aspect ratio with padding
  - **Vertical (9:16)**: 1080x1920, center-cropped from source
- **Quality Settings**: CRF 23, fast preset, AAC audio at 128k
- **Storage**: Clips saved to `clips/{videoId}/` with sanitized filenames

#### 5. **Database Schema**

```prisma
Video (main entity)
  â†“
  â”œâ”€ Transcript (segments with timestamps)
  â”‚    â””â”€ TranscriptChunk (processing chunks)
  â”‚         â””â”€ Highlight (AI-selected moments)
  â”‚
  â””â”€ Clip (generated video files)
       â””â”€ Format (horizontal/vertical)

Job (tracks async processing)
  â””â”€ Types: TRANSCRIPT | HIGHLIGHTS | CLIPS
```

### Technology Stack

| Component | Technology | Why? |
|-----------|-----------|------|
| **Framework** | Next.js 14 | App router, API routes, TypeScript support |
| **Database** | PostgreSQL (Neon) | Relational data, JSON support for segments |
| **ORM** | Prisma | Type-safe queries, migrations |
| **Queue** | BullMQ + Redis | Reliable background job processing |
| **Video Processing** | FFmpeg | Industry-standard video manipulation |
| **AI APIs** | Groq (Whisper + Llama) | Free tier, fast processing, high quality |

### Worker Architecture

**Why BullMQ?**
- Handles long-running transcription jobs (5-10 mins for 1 hour video)
- Retry logic with exponential backoff
- Prevents API timeouts in Next.js routes
- Scalable: can run multiple workers

**Worker Flow:**
```
API creates job â†’ BullMQ queue â†’ Worker picks up â†’ Process â†’ Update DB
```

---

## ğŸ¤– AI API Usage

### Primary AI Provider: **Groq**

#### Why Groq?

**For Transcription (Whisper):**
- âœ… **Free tier**: 30 requests/min, 14,400/day
- âœ… **Fast**: 2-5 seconds per 2-minute chunk
- âœ… **Quality**: Same Whisper model as OpenAI
- âœ… **No billing required**: Perfect for development

**For Highlight Detection (Llama 3.1-8B):**
- âœ… **Free tier**: Generous limits
- âœ… **Fast inference**: ~1-2 seconds per request
- âœ… **JSON mode**: Reliable structured outputs
- âœ… **Good reasoning**: Accurate highlight detection

#### Alternative Considered: OpenAI
- âŒ Requires paid account with billing
- âŒ More expensive per request
- âœ… Slightly better model quality (marginal for this use case)

**Decision**: Groq provides 95% of OpenAI's quality at 0% of the cost, making it ideal for this assignment.

### AI Integration Details

**Whisper Transcription:**
```typescript
// Using Groq's Whisper API
const response = await groq.audio.transcriptions.create({
  file: fs.createReadStream(audioPath),
  model: "whisper-large-v3-turbo",
  language: "en",
});
```

**Highlight Detection:**
```typescript
// Multi-phase pipeline with Llama 3.1
const completion = await groq.chat.completions.create({
  model: "llama-3.1-8b-instant",
  messages: [
    { role: "system", content: CANDIDATE_SYSTEM_PROMPT },
    { role: "user", content: formattedTranscript }
  ],
  temperature: 0.3, // Lower = more consistent
});
```

---

## ğŸ“Š Key Design Decisions & Trade-offs

### 1. **Chunked Transcription**
**Decision**: Split audio into 120-second chunks

**Why?**
- Groq free tier limits (30 req/min)
- Better error recovery (one chunk fails â‰  entire video fails)
- Parallel processing potential

**Trade-off**: Slight overhead in API calls vs. processing entire file at once

### 2. **Multi-Phase Highlight Detection**
**Decision**: Extract candidates â†’ Rank â†’ Final selection (3 AI calls per video)

**Why?**
- Token limits: Can't send 1-hour transcript in one prompt
- Better accuracy: Specialized prompts for each phase
- Deduplication: Prevents overlapping highlights

**Trade-off**: More API calls, but higher quality results

### 3. **BullMQ Workers**
**Decision**: Separate workers for transcription and clips

**Why?**
- Prevents Next.js API route timeouts (10 min+ for long videos)
- Retry logic for failed jobs
- Scalability: Can run multiple workers

**Trade-off**: Added complexity vs. simpler synchronous processing

### 4. **Local File Storage**
**Decision**: Store videos/clips on filesystem instead of S3

**Why?**
- Simpler setup for assignment (no AWS credentials)
- Faster development iteration
- FFmpeg works directly with local files

**Trade-off**: Not production-ready (would use S3/CDN in real app)

### 5. **No Real-time Updates**
**Decision**: Poll-based status checking instead of WebSockets

**Why?**
- Simpler implementation
- Sufficient UX for this use case
- Avoids WebSocket infrastructure

**Trade-off**: Users must refresh to see status updates

### 6. **Prisma ORM**
**Decision**: Use Prisma instead of raw SQL

**Why?**
- Type safety (catches errors at compile time)
- Easy migrations
- Cleaner query syntax

**Trade-off**: Slight performance overhead vs. raw SQL (negligible for this scale)

---

## ğŸ¯ Assumptions Made

1. **Video Format**: Assumes standard video formats (MP4, MOV) that FFmpeg can handle
2. **Audio Present**: Videos contain audio tracks (required for transcription)
3. **English Language**: Transcription optimized for English content
4. **Single Upload**: One video processed at a time (can be scaled with worker concurrency)
5. **Local Development**: Assumes FFmpeg installed globally on system
6. **File Size**: Videos under 2GB (no explicit size validation yet)

---

## ğŸ“ Project Structure

```
video-to-clips-ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ videos/
â”‚   â”‚       â””â”€â”€ [videoId]/
â”‚   â”‚           â”œâ”€â”€ clips/route.ts      # Clip generation endpoint
â”‚   â”‚           â”œâ”€â”€ highlights/route.ts # Highlight detection
â”‚   â”‚           â””â”€â”€ transcript/route.ts # Transcription trigger
â”‚   â””â”€â”€ page.tsx                        # Main UI (upload form)
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ highlights/
â”‚   â”‚   â”‚   â””â”€â”€ generateHighlights.ts   # Multi-phase AI pipeline
â”‚   â”‚   â””â”€â”€ prompts/
â”‚   â”‚       â””â”€â”€ highlight.system.ts      # AI system prompts
â”‚   â”œâ”€â”€ chunks/
â”‚   â”‚   â”œâ”€â”€ createRecords.ts            # Chunk metadata creation
â”‚   â”‚   â””â”€â”€ processChunks.ts            # Chunk transcription
â”‚   â”œâ”€â”€ clips/
â”‚   â”‚   â”œâ”€â”€ generateClip.ts             # FFmpeg clip generation
â”‚   â”‚   â”œâ”€â”€ handleClipJob.ts            # Clip job orchestration
â”‚   â”‚   â””â”€â”€ saveClipRow.ts              # Clip metadata storage
â”‚   â”œâ”€â”€ media/
â”‚   â”‚   â”œâ”€â”€ extractAudio.ts             # Video â†’ Audio conversion
â”‚   â”‚   â””â”€â”€ splitAudio.ts               # Audio chunking
â”‚   â”œâ”€â”€ queue/
â”‚   â”‚   â”œâ”€â”€ clips.queue.ts              # BullMQ clips queue
â”‚   â”‚   â””â”€â”€ redis.ts                    # Redis connection
â”‚   â”œâ”€â”€ transcription/
â”‚   â”‚   â””â”€â”€ sendChunkToWhisper.ts       # Groq Whisper API call
â”‚   â”œâ”€â”€ worker/
â”‚   â”‚   â”œâ”€â”€ transcript.worker.ts        # Transcription worker
â”‚   â”‚   â””â”€â”€ clips.worker.ts             # Clip generation worker
â”‚   â””â”€â”€ prisma.ts                       # Prisma client singleton
â”œâ”€â”€ prisma/
â”‚   â””â”€â”€ schema.prisma                   # Database schema
â”œâ”€â”€ uploads/                            # Uploaded videos
â”œâ”€â”€ clips/                              # Generated clips
â””â”€â”€ README.md
```

---

## ğŸ¬ Usage Flow

1. **Upload Video**: POST to `/api/videos/upload`
2. **Start Transcription**: POST to `/api/videos/{videoId}/transcript`
3. **Generate Highlights**: POST to `/api/videos/{videoId}/highlights`
4. **Create Clips**: POST to `/api/videos/{videoId}/clips`
5. **Download Clips**: Files available in `clips/{videoId}/`

---

## ğŸ”® Future Enhancements

- [ ] Auto-generated captions burned into clips
- [ ] Cloud storage (S3) for videos/clips
- [ ] Real-time progress updates (WebSockets)
- [ ] Clip preview thumbnails
- [ ] Batch video processing
- [ ] Advanced highlight customization (manual time adjustment)
- [ ] Multiple AI model support (switch between providers)

---

## ğŸ“ License

MIT

---

## ğŸ™ Acknowledgments

Built using AI assistance (Claude, Cursor) for:
- FFmpeg command generation
- Error handling patterns
- Database schema design
- Worker architecture decisions

This project demonstrates effective AI-augmented development while maintaining clean architecture and thoughtful design decisions.